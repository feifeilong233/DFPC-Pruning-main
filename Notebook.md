## 无偏和有偏

reward越少模型能力越强，非环境reward99%有偏。
从寻路环境来讲，势能reward无偏。

非独立reward需严格证明有效性，reward互相之间大概率存在互斥，每加一个reward都要和原来所有的reward互相证明有效。

## MSE和L1

在强化学习（RL）中，**均方误差（MSE, Mean Squared Error）** 和 **L1损失（L1 Loss）** 都可以用来衡量模型的预测值与目标值之间的差异，但它们的计算方式和特性不同，适用于不同的场景和需求。下面我们从概念、计算公式、特性和在 RL 中的应用场景来对比它们的区别。

---

### **1. 基本定义与计算公式**

#### **(1) 均方误差（MSE, Mean Squared Error）**
\[
\text{MSE} = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2
\]
- **含义**: 衡量预测值 \( \hat{y}_i \) 和目标值 \( y_i \) 之间的平方误差。
- **特性**:
  - 对大误差非常敏感，因为平方操作会放大较大的偏差（比如异常值）。
  - 对于平滑的误差分布较为有效。
  - 统计学上，一维分布的$y_i$是均值。

#### **(2) L1 损失**
\[
\text{L1 Loss} = \frac{1}{N} \sum_{i=1}^{N} |y_i - \hat{y}_i|
\]
- **含义**: 衡量预测值 \( \hat{y}_i \) 和目标值 \( y_i \) 之间的绝对误差。
- **特性**:
  - 对异常值的鲁棒性较强，因为绝对值对较大偏差没有像平方那样敏感。
  - 可以产生稀疏的梯度（因为绝对值函数的导数是分段常数）。
  - 统计学上，一维分布的$y_i$是中位数。

---

### **2. 在强化学习中的应用场景**

#### **(1) MSE 在强化学习中的应用**
MSE 常用于强化学习中的目标函数，尤其是在处理连续动作空间和回归任务时。

- **典型应用**:
  - **Q-learning 和 DDPG**:
    MSE 常用于目标 Q 值和预测 Q 值之间的差异计算：
    \[
    \text{Loss} = \frac{1}{N} \sum_{i=1}^{N} \left( r_i + \gamma \max_{a'} Q(s', a'; \theta') - Q(s, a; \theta) \right)^2
    \]
    - \( r_i \): 即时奖励。
    - \( \gamma \): 折扣因子。
    - \( Q(s, a; \theta) \): Q 网络的预测值。

- **优势**:
  - 对于较小误差的梯度较大，能够快速收敛。
  - 对平滑目标值（如 Q 值分布）更敏感。

- **劣势**:
  - 对噪声或异常值（如回报中的异常值或估计值中的错误）较敏感，可能导致不稳定或训练发散。

---

#### **(2) L1 损失在强化学习中的应用**
L1 损失适用于需要对异常值更加鲁棒的场景。

- **典型应用**:
  - **鲁棒性强化学习**:
    在训练过程中，L1 损失对回报或估计误差中的异常值更不敏感，因此可以减少异常值对目标网络或策略网络的干扰。
  - **分布式 Q-learning**:
    如果 Q 值的目标分布具有较大偏差或长尾，L1 损失可能更合适。

- **优势**:
  - 对噪声和异常值具有鲁棒性。
  - 在某些场景下可以产生稀疏的梯度，减少对权重的过度调整。

- **劣势**:
  - 对于小误差的梯度较小，收敛速度可能慢于 MSE。
  - 在目标值光滑时，可能导致不稳定的优化路径。

---

### **3. RL 场景中的选择**

- **MSE 的适用场景**:
  - 误差分布相对平滑，且对异常值不敏感的任务。
  - 优先考虑快速收敛的任务，如大规模离线训练。

- **L1 的适用场景**:
  - 回报函数可能有噪声（如环境的随机性较大）。
  - 需要对长尾分布或异常值鲁棒的任务。

#### **案例比较**
假设我们在训练一个 Q-learning 网络：
- 如果环境中的回报函数包含异常值（如偶尔的极大奖励或极小奖励），使用 MSE 可能导致 Q 值的不稳定更新，而 L1 更鲁棒。
- 如果回报函数平稳（如连续的奖励分布），MSE 可以提供更快速的梯度下降路径。

---

### **4. 总结**

| 特性        | MSE                          | L1                         |
|-------------|------------------------------|----------------------------|
| **敏感性**  | 对大误差敏感                 | 对大误差鲁棒               |
| **收敛速度**| 小误差时收敛更快             | 收敛速度可能较慢           |
| **异常值**  | 对异常值敏感，可能导致不稳定  | 对异常值鲁棒               |
| **适用场景**| 平滑误差分布的任务           | 含噪声或异常值的任务       |

在强化学习中，MSE 通常用于标准任务，而 L1 在需要对异常值鲁棒的情况下更为合适。

## Policy Gradient & A2C


